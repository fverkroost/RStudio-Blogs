---
title: "A comparison of methods for predicting clothing classes using the Fashion MNIST dataset in RStudio and Python (Part 3)"
author: "Florianne Verkroost"
date: "23/08/2019"
output: html_document
---

```{r setup, message = FALSE,  echo = FALSE, warning = FALSE, results = 'hide'}
python3_path <- "/Library/Frameworks/Python.framework/Versions/3.7/bin/python3"
knitr::opts_chunk$set(echo = TRUE, engine.path = list(python = python3_path))
library(reticulate)    # run Python in R
library(devtools)
library(tensorflow)    # backend for keras
install_tensorflow()
library(keras)         # obtaining the data
install_keras()        
library(reshape2)      # melting data into long format
library(plyr)          # data manipulation
library(dplyr)         # data manipulation
library(fpc)           # cluster validation statistics
library(ggplot2)       # visualize and plot
library(rpart)         # run single tree models
library(randomForest)  # run random forest models
library(gbm)           # run boosting models
library(e1071)         # run support vector machines
library(caret)         # calculate variable importance
use_python(python3_path)
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
set.seed(1234)
```

```{r, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE, error = FALSE}
fashion_mnist <- keras::dataset_fashion_mnist()
c(train.images, train.labels) %<-% fashion_mnist$train
c(test.images, test.labels) %<-% fashion_mnist$test
train.images <- data.frame(t(apply(train.images, 1, c))) / 255
test.images <- data.frame(t(apply(test.images, 1, c))) / 255
names(train.images) <- names(test.images) <- paste0('pixel', 1:784)
train.labels <- data.frame(label = factor(train.labels))
test.labels <- data.frame(label = factor(test.labels))
train.data <- cbind(train.labels, train.images)
test.data <- cbind(test.labels, test.images)
train.data.subsample <- train.data %>% 
  group_by(label) %>% 
  sample_n(size = 100) %>% 
  ungroup()
train.images.subsample <- train.data.subsample %>% select(-one_of("label"))
train.labels.subsample <- train.data.subsample %>% select(one_of("label"))
```

In this series of blog posts, I will compare different machine and deep learning methods to predict clothing categories from images using the Fashion MNIST data. In the [first blog post](https://github.com/fverkroost/RStudio-Blogs/blob/master/machineLearningFashionMNIST1.Rmd) of the series, we will explore and prepare the data for analysis. I will also show you how to perform K-means clustering as an exploratory analysis, to get an idea of what categories are more similar and dissimilar and thus more or less likely to be misclassified. In the [second blog post](https://github.com/fverkroost/RStudio-Blogs/blob/master/machineLearningFashionMNIST2.Rmd), I will show you how to predict the clothing categories of the Fashion MNIST data using my go-to model: an artificial neural network. Here, I will also show you how to use one of RStudios incredible features to run Python from RStudio. In this third blog post, we will experiment with tree-based methods (single tree, random forests and boosting) and support vector machines to see whether we can beat the neural network in terms of performance. 

## Tree-Based Methods

In this sub-section, we compare different tree-based methods: a single tree, random forest and boosting. Tree-based methods stratify or segment the predictor space into a number of simple regions using a set of decision rules that can be summarized in a decision tree. The focus here will be on classification trees here, as the Fashion MNIST outcome variable is categorical. Unfortunately, single trees have a relatively low level of predictive accuracy compared to other classification approaches, such as logistic regression or discriminant analysis. To improve prediction accuracy, ensemble methods like random forests and boosting aggregate many single decision trees and thereby provide a good way to achieve prediction accuracy while decreasing variance. Here, I decided to show random forests and boosting as the former are easier to implement as they are more robust to over-fitting and require less tuning, while the latter generally outperforms other tree-based methods in terms of prediction accuracy. These models are run in supervised mode here as labeled data is available and the goal is to predict classes. For a more formal explanation of the methods used in this blog, I refer you to 'An Introduction to Statistical Learning' by James, Witten, Hastie and Tibshirani (2011).


Before we start, we define a simple function *accuracy* that we can easily use for all models to calculate the training and test set accuracies.

```{r}
accuracy <- function(fit, train_data, test_data, label, model_name){
  
  pred_train <- predict(fit, train_data)
  pred_test <- predict(fit, test_data)
  
  if (class(pred_train) %in% c("matrix", "array", "data.frame")){
    pred_train <- apply(pred_train, 1, which.max) - 1
  }
  if (class(pred_test) %in% c("matrix", "array", "data.frame")){
    pred_test <- apply(pred_test, 1, which.max) - 1
  }
  
  conf_train <- table(train_data[, label], pred_train)
  acc_train <- sum(diag(conf_train)) / sum(conf_train)
  
  conf_test <- table(test_data[, label], pred_test)
  acc_test <- sum(diag(conf_test)) / sum(conf_test)
  
  acc <- data.frame(acc_train = acc_train, acc_test = acc_test, model = model_name)
  print(acc)
  
  return(acc)
}
```

# Single Tree

We start by fitting a single classification tree. It is important to make sure that the *label* variable in the data are of *factor* class such that a classification tree is fit.

```{r}
tree <- rpart(label ~., method = "class", data = train.data)
plotcp(tree)
printcp(tree)
acc.single.tree <- accuracy(tree, train.data, test.data, 'label', 'single_tree')
```

Our tree is made up of `r length(tree$variable.importance)` variables and contains `r nrow(tree$frame)` nodes and `r length(unique(tree$where))` terminal nodes. For single decision trees, the resulting tree that minimizes impurity and variance is likely to be too complex and to over-fit the training data. Tree pruning can be used to find a smaller tree with fewer splits that introduces some bias for the sake of interpretability and variance reduction. For single trees, cost-complexity pruning is usually used to improve the performance of the model.

```{r}
prune.tree <- prune(tree, cp = tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"])
acc.pruned.tree <- accuracy(tree, train.data, test.data, 'label', 'pruned_tree')
```

To see what both trees look like, we plot them side by side.

```{r}
par(mfrow = c(1, 2))
plot(tree, uniform = TRUE, main = "Classification Tree")
text(tree, cex = 0.5)
plot(prune.tree, uniform = TRUE, main = "Pruned Classification Tree")
text(prune.tree, cex = 0.5) 
```

Comparing the accuracies of the single and pruned trees, we observe that the original tree actually performs worse than the pruned tree.

# Random Forest

The single unpruned and pruned classification trees do not perform very well. Random forests use bootstrap aggregating to the reduce variance of the outcomes. First, bootstrapping (sampling with replacement) is used to create B training sets from the population with the same size as the original training set. Second, a separate tree for each of these training sets is build. Trees are grown using recursive binary splitting on the training data until a node reaches some minimum number of observations. Hereby, the tree should go from impure (equal mixing of classes) to pure (each leaf corresponds to one class exactly). The splits are determined such that they decrease variance, error and impurity. Random forests decorrelate the trees by considering only m of all p predictors as split candidates, whereby m is usually chosen to equal the square root of p. Classification trees predict that each observation belongs to the most commonly occurring class (i.e. majority vote) of training observations in the region to which it belongs. The classification error rate is the fraction of the number of misclassified observations and the total number of classified observations. The Gini index and cross-entropy measures determine the level of impurity in order to determine the best split at each node.Third, the average of the classification prediction results of all B trees is computed from the majority vote, i.e. the prediction that occurs most often among the B predictors. The accuracy is computed as the out-of-bag (OOB) error and/or the test set error (in case a test set is provided). As each bootstrap samples from the training set with replacement, about 2/3 of the observations are not sampled and some are sampled multiple times. In case of B trees in the forest, each observation is left out of approximately B/3 trees. The non-sampled observations are used as test set and the B/3 trees are used for out-of-sample predictions. In random forests, pruning is not needed as potential over-fitting is (partially) mitigated by the usage of bootstrapped samples and multiple decorrelated random trees. 

We start by tuning the parameters for the random forest.

```{r, message = FALSE, error = FALSE, warning = FALSE}
tuning <- tuneRF(train.images.subsample, train.data.subsample$label,doBest = TRUE)
rf <- randomForest(x = train.images.subsample, y = train.data.subsample$label,
                   xtest = test.images, ytest = test.data$label,
                   keep.forest = TRUE, mtry = tuning$mtry, importance = TRUE)
rf
```

Next, we can look at the results and variable importance as well as the Gini index and cross-entropy.

```{r}
plot(rf, main = "Relation between error and random forest size")
varImpPlot(rf)
imp.acc <- sort(rf$importance[, 11], decreasing = TRUE)
imp.gini <- sort(rf$importance[, 12], decreasing = TRUE)
```

We finally compute the accuracy achieved by the random forest.

```{r}
acc.rf <- accuracy(rf, train.data, test.data, 'label', 'random_forest')
```

We can also plot the confusion matrix to make it easier to interpret the results.

```{r message = FALSE}
conf <- rf$test$confusion[, -ncol(rf$test$confusion)]
conf <- data.frame(conf / rowSums(conf))
conf["true"] <- rownames(conf)
conf.melt <- melt(conf, by = "true")
conf.melt$variable <- gsub("X", "", conf.melt$variable)
g <- ggplot()
g <- g + geom_tile(data = conf.melt, aes(x = true, y = variable, fill = value))
g <- g + labs(x = "Actual", y = "Predicted", fill = "Percent")
g
```

We observe from this plot that most of the classes are predicted accurately as the light blue (high percentages of correct predictions) are on the diagonal of the tile plot. We can also observe that category six (shirts) are quite often misclassified as categories 0 (t-shirt/top), 2 (pullover), 3 (dress) and 4 (coat), which makes sense because these are all mostly upper body clothing parts having similar shapes.

# Boosting

Where in random forests each tree is fully grown and trained independently with a random sample of data, in boosting every newly built tree incorporates the error from the previously built tree. That is, the trees are grown sequentially on an adapted version of the initial data, which does not require bootstrap sampling. Because of this, boosted trees are usually smaller and more shallow than the trees in random forests, improving the tree where it does not work well enough yet. Boosting is often said to outperform random forests, which is mainly because the approach learns slowly, which can be even further controlled by one of its parameters (i.e. shrinkage), which we'll tune later. Let's start with running a model where most of the parameters are at their default value.

```{r message = FALSE, warning = FALSE, results = 'hide'}
boost.notune <- gbm(label ~., data = train.data.subsample, shrinkage = .001, 
                       distribution = "multinomial", cv.folds = 10, n.trees = 200, 
                       n.minobsinnode = 10, interaction.depth = 4, 
                       bag.fraction = 0.5, train.fraction = 1)
```
```{r}
acc.boost.notune <- accuracy(boost.notune, train.data, 
                             test.data, 'label', 'boost_no_tune')
```

In boosting, it's important to tune the parameters well and play around with different values of the parameters, which include the learning rate (shrinkage), the number of trees to be grown (n.trees), the number of splits to perform on a tree (interaction.depth), the minimum number of observsations in the trees' terminal nodes (n.minobsinnode), the fraction of the training set observations randomly selected to propose the next tree in the expansion (bag.fraction) and the fraction of observations to use to fit the model when the remainder is used to compute out-of-sample estimates (train.fraction). As our training subsample is relatively small (1000 observations), we increase the bag.fraction and decrease the n.minobsinnode. We further increase the number of trees, shrinkage and interaction depth to improve model performance.

```{r message = FALSE, warning = FALSE, results = 'hide'}
boost.tune <- gbm(label ~., data = train.data.subsample, shrinkage = .01,
                      distribution = "multinomial", cv.folds = 10, n.trees = 500, 
                      n.minobsinnode = 5, interaction.depth = 6, 
                      bag.fraction = 0.8, train.fraction = 1)
```
```{r}
acc.boost.tune <- accuracy(boost.tune, train.data, test.data, 'label', 'boost_tune')
```

From the results, we observe that in this case increasing shrinkage improves both training and test sey accuracy. Next, we can look at the variable importance, which we do visually.

```{r}
var.imp <- varImp(boost.tune, numTrees = 100)
var.imp["pixel"] <- rownames(var.imp)
var.imp <- var.imp[order(var.imp$Overall, decreasing = TRUE), ]
g <- ggplot()
g <- g + geom_bar(data = var.imp[1:10, ], stat = "identity",
                  aes(x = reorder(pixel, -Overall), y = Overall))
g <- g + theme(panel.grid.major = element_blank(), 
               panel.grid.minor = element_blank(),
               panel.background = element_blank(), 
               axis.line = element_line(colour = "black"))
g <- g + labs(x = NULL, y = "Relative importance")
g
```


The plot shows what pixels are most important for classification. Remembering our original pictures are 28 by 28 pixels which we've vectorized, we can see that the most important pixels for predicting clothing categories are most often pixels in the middle of the figure. For example, pixel 410 is the [14, 18] entry in the 28 by 28 pixel picture.


## Support Vector Machine

For an n by p data matrix and binary outcome variable, a hyperplane is a flat affine subspace of dimension p - 1 that divides the p-dimensional space in two halves. A test observation is assigned to a class depending on which side of the perfectly (or if not existent, almost) seperating hyperplane it lies. If an observationâ€™s score is smaller than cutoff value t, it belongs to one class; if it is equal to or larger than t, it belongs to the other class. The distance of an observation indicates the confidence about the class assignment: the further an observation is located from zero (and thus from the hyperplane), the more confidence. If existent, an infinite number of separating hyperplanes can be constructed. The maximal margin classifier (MMC), which is in essence the mid-line of the widest strip inserted between two classes, is a good option. If a perfectly separating hyperplane does not exist, "almost separating" hyperplanes can be used by means of a soft-margin or support vector classifier (SVC). The SVC extends the MMC as it does not require classes to be separable by a linear boundary by including slack variables epsilon that allow some observations to be on the incorrect side of the margin or hyperplane. The extent to which incorrect placements are done is determined by tuning parameter cost, which controls the bias-variance trade-off. 

Both MMCs and SVCs assume a linear boundary between the two classes of the outcome variable. Non-linearity can be addressed by enlarging the feature space using functions of predictors. Support vector machines combine support vector classifiers with non-linear Kernels (e.g. radial, polynomial or sigmoid) to achieve efficient computations. Kernels are generalizations of inner products that quantify the similarity of two observations. Usually, the radial Kernel is selected for non-linear models as it provides a good default Kernel in the absence of prior knowledge of invariances regarding translations. The radial Kernel is partially determined by gamma, a positive constant that makes the fit more non-linear as it increases. Tuning the cost and gamma parameters is necessary to find the optimal trade-off between reducing the number of training errors and making the decision boundary more irregular (by increasing the cost). As SVMs only require the computation of n choose k Kernels for all distinct observation pairs, they greatly improve efficiency. 

We start by fitting an SVM with linear Kernel and gamma and cost parameters equal to one, to compare our tuned results with later on.

```{r message = FALSE, warning = FALSE, results = 'hide'}
svm.lin <- svm(label ~., data = train.data.subsample, kernel = "linear", 
               gamma = 1, cost = 1, decision.values = TRUE)
acc.svm.lin <- accuracy(svm.lin, train.data, test.data, 'label', 'svm_lin')
```

Next, we tune the cost, gamma and epsilon parameters using the *tune* function.

```{r message = FALSE, warning = FALSE, results = 'hide'}
tune.out <- tune.svm(label ~., data = train.data.subsample, kernel = "radial", 
                 gamma = 10^(-5:1), cost = 10^(-3:1))
summary(tune.out)
```

The results show that the optimal parameters for gamma, cost and epsilon are, `r tune.out$best.model$gamma`, `r tune.out$best.model$cost` and `r tune.out$best.model$epsilon`, respectively, and we use these to fit our optimal model.

```{r message = FALSE, warning = FALSE, results = 'hide'}
svm.opt <- svm(label ~., data = train.data.subsample, kernel = "radial", 
               gamma = tune.out$best.model$gamma, cost = tune.out$best.model$cost, 
               epsilon = tune.out$best.model$epsilon, decision.values = TRUE)
pred.opt <- predict(svm.opt, train.data.subsample, decision.values = TRUE)
fit.opt <- attributes(pred.opt)$decision.values
acc.svm.opt <- accuracy(svm.opt, train.data, test.data, 'label', 'svm_opt')
```

The results show that the optimal tuned model with radial Kernel has higher training and test set accuracy than the untuned model with linear Kernel. The confusion matrix shows that, as we saw before with the random forest results, categories six (shirts) and four (coats) are most often wrongly classified, with coats often being misclassified as pullovers (2) or shirts (6) and shirts often being misclassified as t-shirt/top (0), pullover (2) or coat (4). These results make sense because these categories are most similar and alike in terms of shape, whereas shoes or trousers for example have completely different shapes and are therefore easier to distinguish bewteen.

```{r}
table(test.data$label, predict(svm.opt, test.data))
```


To recap, the accuracies we have obtained so far from tree-based methods an support vector machine are the following:

```{r}
rbind.fill(list(acc.single.tree, acc.pruned.tree, acc.rf, acc.boost.notune,
                acc.boost.tune, acc.svm.lin, acc.svm.opt))
```

From this table, we observe that our optimal tuned support vector machine performs best on the test data set (80.54 percent accuracy), followed by the random forest (79.83 percent accuracy) and the tuned boosting model (78.96 percent accuracy). Nevertheless, all of these models are outperformed by the neural netowrk, achieving almost 90 percent accuracy.

## Wrapping Up

In this series of blog posts, we have aimed to predict the clothing categories of the Fashion MNIST data set from their 28 by 28 pixel images. We have used supervised and unsupervised learning methods, inclusing K-means clustering, tree-based methods, support vector machines and neural networks. In addition, this blog showed how to incorporate and run Python code in RStudio either via Terminal or the *system2* function. Random forest, boosting and support vector machine showed similar results in terms of test set accuracy, all achieving about 80 percent accuracy after tuning. However, the neural networks, even the single-hidden-layer network, outperformed these models, achieving almost 90 percent accuracy. 

